{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11178614,"sourceType":"datasetVersion","datasetId":6977246},{"sourceId":230605938,"sourceType":"kernelVersion"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Sequential\nfrom tensorflow.keras.applications import EfficientNetV2S\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.mixed_precision import set_global_policy\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.utils.class_weight import compute_class_weight\nimport matplotlib.pyplot as plt\nfrom glob import glob\nimport random\nfrom PIL import Image\nimport seaborn as sns\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import mixed_precision\n\nprint(\"TensorFlow Version:\", tf.__version__)\nprint(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\nmixed_precision.set_global_policy(\"mixed_float16\")  # Speed up training on GPU\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-31T11:53:24.330401Z","iopub.execute_input":"2025-03-31T11:53:24.330681Z","iopub.status.idle":"2025-03-31T11:53:37.550054Z","shell.execute_reply.started":"2025-03-31T11:53:24.330650Z","shell.execute_reply":"2025-03-31T11:53:37.549256Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# Dataset directory path\ndirectory_path = \"/kaggle/input/ppd-v2/ppd_v1\"\n\n# Dictionary to store class names and image counts\nclass_counts = {}\n\n# Iterate over each class folder\nfor class_name in os.listdir(directory_path):\n    class_path = os.path.join(directory_path, class_name)\n    if os.path.isdir(class_path):  # Ensure it's a folder\n        image_count = len([f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))])\n        class_counts[class_name] = image_count\n\n# Convert to DataFrame for better visualization\ndf = pd.DataFrame(list(class_counts.items()), columns=[\"Class\", \"Image Count\"])\n\n# Display results\nprint(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T11:53:44.686357Z","iopub.execute_input":"2025-03-31T11:53:44.686658Z","iopub.status.idle":"2025-03-31T11:54:12.984990Z","shell.execute_reply.started":"2025-03-31T11:53:44.686632Z","shell.execute_reply":"2025-03-31T11:54:12.984099Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Summary Statistics","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Display basic statistics\nprint(df.describe())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T11:54:16.485785Z","iopub.execute_input":"2025-03-31T11:54:16.486086Z","iopub.status.idle":"2025-03-31T11:54:16.500604Z","shell.execute_reply.started":"2025-03-31T11:54:16.486063Z","shell.execute_reply":"2025-03-31T11:54:16.499819Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Distribution of Image Counts","metadata":{}},{"cell_type":"code","source":"# Set plot size\nplt.figure(figsize=(14, 6))\n\n# Bar plot of class distributions\nsns.barplot(x=\"Class\", y=\"Image Count\", data=df, palette=\"viridis\")\n\n# Rotate x-axis labels for readability\nplt.xticks(rotation=90)\n\n# Labels and title\nplt.xlabel(\"Class\")\nplt.ylabel(\"Number of Images\")\nplt.title(\"Image Distribution Across Classes\")\n\n# Show the plot\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T11:54:31.904986Z","iopub.execute_input":"2025-03-31T11:54:31.905303Z","iopub.status.idle":"2025-03-31T11:54:32.258260Z","shell.execute_reply.started":"2025-03-31T11:54:31.905277Z","shell.execute_reply":"2025-03-31T11:54:32.257360Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Check All Image Extensions in the Dataset","metadata":{}},{"cell_type":"code","source":"# Set to store unique file extensions\nunique_extensions = set()\n\n# Loop through each class folder\nfor pest_class in os.listdir(directory_path):\n    class_dir = os.path.join(directory_path, pest_class)\n    if os.path.isdir(class_dir):  # Ensure it's a directory\n        for file in os.listdir(class_dir):\n            ext = os.path.splitext(file)[1].lower()  # Extract file extension\n            if ext:  # Ensure there's an extension\n                unique_extensions.add(ext)\n\n# Display all unique extensions found\nprint(\"ðŸ“Œ Unique file extensions in the dataset:\", unique_extensions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T11:54:37.673531Z","iopub.execute_input":"2025-03-31T11:54:37.673873Z","iopub.status.idle":"2025-03-31T11:54:37.705983Z","shell.execute_reply.started":"2025-03-31T11:54:37.673846Z","shell.execute_reply":"2025-03-31T11:54:37.705002Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Display 2 Images Per Class","metadata":{}},{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nimport random\nimport cv2\nimport numpy as np\n\n# Dataset directory\ndirectory_path = \"/kaggle/input/ppd-v2/ppd_v1\"\n\n# Number of samples per class\nsamples_per_class = 2\n\n# Get all class names\nclass_names = [name for name in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, name))]\n\n# Define number of rows and columns\nnum_rows = len(class_names)\nnum_cols = samples_per_class\n\n# Set figure size\nfig, axes = plt.subplots(num_rows, num_cols, figsize=(12, num_rows * 2))\n\n# Iterate over each class\nfor row_idx, class_name in enumerate(class_names):\n    class_path = os.path.join(directory_path, class_name)\n    \n    # Get all images in the class directory\n    image_files = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))]\n    \n    # Select 2 random images\n    selected_images = random.sample(image_files, min(samples_per_class, len(image_files)))\n    \n    # Display images\n    for col_idx, image_file in enumerate(selected_images):\n        img_path = os.path.join(class_path, image_file)\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n        \n        ax = axes[row_idx, col_idx]\n        ax.imshow(img)\n        ax.axis(\"off\")\n        ax.set_title(class_name if col_idx == 0 else \"\")\n\n# Add separation lines\nfor row in range(num_rows - 1):  # Horizontal lines\n    plt.axhline(y=row + 0.5, color='black', linewidth=2, linestyle=\"--\")\n\nfor col in range(num_cols - 1):  # Vertical lines\n    plt.axvline(x=col + 0.5, color='black', linewidth=2, linestyle=\"--\")\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T11:54:41.691929Z","iopub.execute_input":"2025-03-31T11:54:41.692353Z","iopub.status.idle":"2025-03-31T11:54:49.678712Z","shell.execute_reply.started":"2025-03-31T11:54:41.692317Z","shell.execute_reply":"2025-03-31T11:54:49.677554Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Overlooking on the Dataset","metadata":{}},{"cell_type":"code","source":"pest_classes = os.listdir(directory_path)\nprint(f\"ðŸ“Œ Found {len(pest_classes)} classes: {pest_classes}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T11:55:04.003578Z","iopub.execute_input":"2025-03-31T11:55:04.003922Z","iopub.status.idle":"2025-03-31T11:55:04.009000Z","shell.execute_reply.started":"2025-03-31T11:55:04.003894Z","shell.execute_reply":"2025-03-31T11:55:04.008266Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"VALID_EXTENSIONS = {\".jpg\", \".jpeg\", \".png\", \".webp\", \".jfif\", \".gif\"}  # Valid image formats\n\n# Dictionary to store image counts per class\nimage_counts = {}\n\n# Loop through each class folder\nfor pest_class in os.listdir(directory_path):\n    class_dir = os.path.join(directory_path, pest_class)\n    if os.path.isdir(class_dir):  # Ensure it's a directory\n        num_images = sum(\n            len(glob(os.path.join(class_dir, f\"*{ext}\"))) for ext in VALID_EXTENSIONS\n        )\n        image_counts[pest_class] = num_images\n\n# Convert to DataFrame for better visualization\ndf_counts = pd.DataFrame(image_counts.items(), columns=[\"Class\", \"Image Count\"])\n\n# Display total images in the dataset\ntotal_images = df_counts[\"Image Count\"].sum()\nprint(f\"ðŸ“Œ Total images in dataset: {total_images}\")\ndf_counts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T11:55:06.950893Z","iopub.execute_input":"2025-03-31T11:55:06.951184Z","iopub.status.idle":"2025-03-31T11:55:07.113384Z","shell.execute_reply.started":"2025-03-31T11:55:06.951162Z","shell.execute_reply":"2025-03-31T11:55:07.112701Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **DL model to do the automatic pest detection**","metadata":{}},{"cell_type":"markdown","source":"## Pest classification using an EfficientNetV2 model\nSince each image contain one kind of pest, we will try using classification with EfficientNetV2","metadata":{}},{"cell_type":"markdown","source":"### Data preparation","metadata":{}},{"cell_type":"code","source":"# Dataset Path\nDATASET_PATH = \"/kaggle/input/ppd-v2/ppd_v1\"\nIMG_SIZE = (224, 224)\nBATCH_SIZE = 32\n\n# Data Augmentation\ndata_gen = ImageDataGenerator(\n    rescale=1.0/255.0,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    validation_split=0.2\n)\n\n# Train & Validation Data\ntrain_data = data_gen.flow_from_directory(\n    DATASET_PATH, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', subset='training'\n)\nval_data = data_gen.flow_from_directory(\n    DATASET_PATH, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', subset='validation'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T11:55:11.751959Z","iopub.execute_input":"2025-03-31T11:55:11.752274Z","iopub.status.idle":"2025-03-31T11:55:16.540494Z","shell.execute_reply.started":"2025-03-31T11:55:11.752246Z","shell.execute_reply":"2025-03-31T11:55:16.539886Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model setup","metadata":{}},{"cell_type":"code","source":"# Load Pretrained EfficientNetV2S\nbase_model = tf.keras.applications.EfficientNetV2S(\n    weights='imagenet',\n    include_top=False,\n    input_shape=(224, 224, 3)\n)\nbase_model.trainable = False  # Freeze initially\n\n# Ensure BatchNorm Layers Stay Frozen\nfor layer in base_model.layers:\n    if isinstance(layer, keras.layers.BatchNormalization):\n        layer.trainable = False\n\n# Classification Head\nmodel = keras.Sequential([\n    base_model,\n    keras.layers.GlobalAveragePooling2D(),\n    keras.layers.Dense(512, activation='relu'),\n    keras.layers.Dropout(0.4),  # Increased dropout\n    keras.layers.Dense(train_data.num_classes, activation='softmax')\n])\n\n# Compile Model with Lower LR\nmodel.compile(\n    optimizer=keras.optimizers.AdamW(learning_rate=1e-4),  # Lowered LR\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T11:55:22.433492Z","iopub.execute_input":"2025-03-31T11:55:22.433836Z","iopub.status.idle":"2025-03-31T11:55:27.034056Z","shell.execute_reply.started":"2025-03-31T11:55:22.433806Z","shell.execute_reply":"2025-03-31T11:55:27.033161Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"# Callbacks\nearly_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\nlr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n\n# Training\nhistory = model.fit(\n    train_data,\n    validation_data=val_data,\n    epochs=30,\n    steps_per_epoch=train_data.samples // BATCH_SIZE,\n    validation_steps=val_data.samples // BATCH_SIZE,\n    callbacks=[early_stopping, lr_scheduler]\n)\n\n# Unfreeze the Top Layers for Fine-Tuning\nbase_model.trainable = True\nfor layer in base_model.layers[:100]:  # Freeze bottom layers, unfreeze top layers\n    layer.trainable = False\n\n# Recompile Model with Lower LR\nmodel.compile(\n    optimizer=keras.optimizers.AdamW(learning_rate=1e-5),  # Even lower LR for fine-tuning\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Fine-tune Model\nhistory_fine = model.fit(\n    train_data,\n    validation_data=val_data,\n    epochs=10,\n    steps_per_epoch=train_data.samples // BATCH_SIZE,\n    validation_steps=val_data.samples // BATCH_SIZE,\n    callbacks=[early_stopping, lr_scheduler]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T11:55:33.219019Z","iopub.execute_input":"2025-03-31T11:55:33.219329Z","iopub.status.idle":"2025-03-31T12:26:11.215221Z","shell.execute_reply.started":"2025-03-31T11:55:33.219302Z","shell.execute_reply":"2025-03-31T12:26:11.214234Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Evaluation","metadata":{}},{"cell_type":"code","source":"def plot_history(history):\n    plt.figure(figsize=(12, 4))\n    \n    # Accuracy Plot\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['accuracy'], label='Train Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n    plt.legend()\n    plt.title(\"Model Accuracy\")\n    \n    # Loss Plot\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'], label='Train Loss')\n    plt.plot(history.history['val_loss'], label='Val Loss')\n    plt.legend()\n    plt.title(\"Model Loss\")\n    \n    plt.show()\n\nplot_history(history)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T12:42:21.572241Z","iopub.execute_input":"2025-03-31T12:42:21.572560Z","iopub.status.idle":"2025-03-31T12:42:21.936984Z","shell.execute_reply.started":"2025-03-31T12:42:21.572537Z","shell.execute_reply":"2025-03-31T12:42:21.936132Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model inference","metadata":{}},{"cell_type":"code","source":"def predict_image(image_path, model):\n    img = keras.preprocessing.image.load_img(image_path, target_size=IMG_SIZE)\n    img_array = keras.preprocessing.image.img_to_array(img) / 255.0\n    img_array = np.expand_dims(img_array, axis=0)\n    \n    predictions = model.predict(img_array)\n    class_idx = np.argmax(predictions[0])\n    class_name = list(train_data.class_indices.keys())[class_idx]\n    confidence = predictions[0][class_idx]\n    \n    plt.imshow(img)\n    plt.axis(\"off\")\n    plt.title(f\"Predicted: {class_name} ({confidence:.2f})\")\n    plt.show()\n\npredict_image(\"/kaggle/input/ppd-v2/ppd_v1/Empoasca fabae/69483.jpg\", model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T12:44:01.559759Z","iopub.execute_input":"2025-03-31T12:44:01.560083Z","iopub.status.idle":"2025-03-31T12:44:12.168400Z","shell.execute_reply.started":"2025-03-31T12:44:01.560060Z","shell.execute_reply":"2025-03-31T12:44:12.167587Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test 2","metadata":{}},{"cell_type":"code","source":"# Import Data Science Libraries\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nimport itertools\nimport random\n\n# Import visualization libraries\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport cv2\nimport seaborn as sns\n\n# Tensorflow Libraries\nfrom tensorflow import keras\nfrom tensorflow.keras import layers,models\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.callbacks import Callback, EarlyStopping,ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras import Model\nfrom tensorflow.keras import preprocessing\n# System libraries\nfrom pathlib import Path\nimport os.path\n\n# Metrics\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nsns.set(style='darkgrid')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T12:56:51.608174Z","iopub.execute_input":"2025-03-31T12:56:51.608504Z","iopub.status.idle":"2025-03-31T12:56:51.614926Z","shell.execute_reply.started":"2025-03-31T12:56:51.608479Z","shell.execute_reply":"2025-03-31T12:56:51.614019Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def seed_everything(seed=42):\n    # Seed value for TensorFlow\n    tf.random.set_seed(seed)\n    \n    # Seed value for NumPy\n    np.random.seed(seed)\n    \n    # Seed value for Python's random library\n    random.seed(seed)\n    \n    # For TensorFlow 2.x, use these settings for deterministic behavior\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n    \n    # Set global determinism for CUDA operations if using GPU\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \n    # Set thread determinism if you need multiprocessing\n    # Note: This may impact performance\n    tf.config.threading.set_inter_op_parallelism_threads(1)\n    tf.config.threading.set_intra_op_parallelism_threads(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T13:03:52.538507Z","iopub.execute_input":"2025-03-31T13:03:52.538851Z","iopub.status.idle":"2025-03-31T13:03:52.543393Z","shell.execute_reply.started":"2025-03-31T13:03:52.538818Z","shell.execute_reply":"2025-03-31T13:03:52.542633Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n\n# Import series of helper functions for our notebook\nfrom helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, compare_historys, walk_through_dir, pred_and_plot","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T13:04:16.793519Z","iopub.execute_input":"2025-03-31T13:04:16.793852Z","iopub.status.idle":"2025-03-31T13:04:17.328369Z","shell.execute_reply.started":"2025-03-31T13:04:16.793826Z","shell.execute_reply":"2025-03-31T13:04:17.327322Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BATCH_SIZE = 32\nTARGET_SIZE = (224, 224)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T13:04:28.217860Z","iopub.execute_input":"2025-03-31T13:04:28.218196Z","iopub.status.idle":"2025-03-31T13:04:28.222243Z","shell.execute_reply.started":"2025-03-31T13:04:28.218166Z","shell.execute_reply":"2025-03-31T13:04:28.221331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Walk through each directory\ndataset = \"/kaggle/input/ppd-v2/ppd_v1\"\nwalk_through_dir(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T13:04:58.616567Z","iopub.execute_input":"2025-03-31T13:04:58.616951Z","iopub.status.idle":"2025-03-31T13:05:09.668249Z","shell.execute_reply.started":"2025-03-31T13:04:58.616918Z","shell.execute_reply":"2025-03-31T13:05:09.667255Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def convert_path_to_df(dataset):\n    image_dir = Path(dataset)\n    \n    # List of all extensions to search for (in both lowercase and uppercase)\n    extensions = ['.png', '.gif', '.jfif', '.jpg', '.webp', '.jpeg']\n    extensions += [ext.upper() for ext in extensions]\n    \n    # Get filepaths for all specified extensions\n    filepaths = []\n    for ext in extensions:\n        filepaths.extend(list(image_dir.glob(f'**/*{ext}')))\n    \n    # If no images found, return empty DataFrame\n    if not filepaths:\n        return pd.DataFrame(columns=['Filepath', 'Label'])\n    \n    # Extract labels from directory structure\n    labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n    \n    # Convert to Series\n    filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n    labels = pd.Series(labels, name='Label')\n    \n    # Concatenate filepaths and labels\n    image_df = pd.concat([filepaths, labels], axis=1)\n    \n    return image_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T13:06:57.340699Z","iopub.execute_input":"2025-03-31T13:06:57.341031Z","iopub.status.idle":"2025-03-31T13:06:57.346834Z","shell.execute_reply.started":"2025-03-31T13:06:57.341006Z","shell.execute_reply":"2025-03-31T13:06:57.345984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for corrupted images within the dataset\nimport PIL\nfrom pathlib import Path\nfrom PIL import UnidentifiedImageError\n\n# List of all extensions to check\nextensions = ['.png', '.gif', '.jfif', '.jpg', '.webp', '.jpeg']\nextensions += [ext.upper() for ext in extensions]\n\ncorrupted_images = []\nbase_path = Path(\"/kaggle/input/ppd-v2/ppd_v1\")\n\n# Check each extension\nfor ext in extensions:\n    path = base_path.rglob(f\"*{ext}\")\n    for img_p in path:\n        try:\n            img = PIL.Image.open(img_p)\n            # Optional: verify the image by loading it\n            img.verify()\n        except (PIL.UnidentifiedImageError, PIL.Image.DecompressionBombError, OSError, ValueError) as e:\n            print(f\"Corrupted image: {img_p} - Error: {e}\")\n            corrupted_images.append(img_p)\n\nprint(f\"Total corrupted images found: {len(corrupted_images)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T14:03:34.471434Z","iopub.execute_input":"2025-03-31T14:03:34.471764Z","iopub.status.idle":"2025-03-31T14:03:41.431278Z","shell.execute_reply.started":"2025-03-31T14:03:34.471719Z","shell.execute_reply":"2025-03-31T14:03:41.430345Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_counts = image_df['Label'].value_counts()\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=label_counts.index, y=label_counts.values, alpha=0.8, palette='rocket')\nplt.title('Distribution of Labels in Image Dataset', fontsize=16)\nplt.xlabel('Label', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.xticks(rotation=45) \nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T14:03:45.544718Z","iopub.execute_input":"2025-03-31T14:03:45.545082Z","iopub.status.idle":"2025-03-31T14:03:45.845364Z","shell.execute_reply.started":"2025-03-31T14:03:45.545053Z","shell.execute_reply":"2025-03-31T14:03:45.844519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(image_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T14:00:24.668644Z","iopub.execute_input":"2025-03-31T14:00:24.669027Z","iopub.status.idle":"2025-03-31T14:00:24.674009Z","shell.execute_reply.started":"2025-03-31T14:00:24.668996Z","shell.execute_reply":"2025-03-31T14:00:24.673308Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Filtrer le dataset en excluant certaines classes\nexcluded_labels = ['termite', 'fall armyworm', 'locust']\nimage_df = image_df[~image_df['Label'].isin(excluded_labels)].reset_index(drop=True)  # RÃ©initialiser l'index\n\n# GÃ©nÃ©rer de nouveaux indices alÃ©atoires aprÃ¨s filtrage\nrandom_index = np.random.choice(len(image_df), 16, replace=False)\n\n# Affichage des images\nfig, axes = plt.subplots(nrows=4, ncols=4, figsize=(10, 10), subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(image_df.Filepath.iloc[random_index[i]]))  # Utiliser `.iloc` pour Ã©viter les erreurs d'index\n    ax.set_title(image_df.Label.iloc[random_index[i]])\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T14:03:52.784060Z","iopub.execute_input":"2025-03-31T14:03:52.784351Z","iopub.status.idle":"2025-03-31T14:03:54.287244Z","shell.execute_reply.started":"2025-03-31T14:03:52.784329Z","shell.execute_reply":"2025-03-31T14:03:54.286157Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_ela_cv(path, quality):\n    \"\"\"\n    Compute Error Level Analysis using OpenCV\n    \n    Args:\n        path: Path to the input image\n        quality: JPEG compression quality (0-100)\n    \n    Returns:\n        Difference image highlighting compression artifacts\n    \"\"\"\n    temp_filename = 'temp_file_name.jpeg'\n    SCALE = 15\n    \n    # Handle different file formats\n    try:\n        orig_img = cv2.imread(path)\n        if orig_img is None:\n            raise ValueError(f\"Failed to load image: {path}\")\n        \n        orig_img = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)\n        \n        cv2.imwrite(temp_filename, orig_img, [cv2.IMWRITE_JPEG_QUALITY, quality])\n\n        # Read compressed image\n        compressed_img = cv2.imread(temp_filename)\n\n        # Get absolute difference between img1 and img2 and multiply by scale\n        diff = SCALE * cv2.absdiff(orig_img, compressed_img)\n        \n        # Clean up temp file\n        if os.path.exists(temp_filename):\n            os.remove(temp_filename)\n            \n        return diff\n    \n    except Exception as e:\n        print(f\"Error processing {path}: {e}\")\n        # Return a blank image of the same size\n        if 'orig_img' in locals() and orig_img is not None:\n            return np.zeros_like(orig_img)\n        return None\n\n\ndef convert_to_ela_image(path, quality):\n    \"\"\"\n    Convert image to Error Level Analysis image using PIL\n    \n    Args:\n        path: Path to the input image\n        quality: JPEG compression quality (0-100)\n    \n    Returns:\n        PIL Image showing ELA result\n    \"\"\"\n    temp_filename = 'temp_file_name.jpeg'\n    \n    try:\n        # Open and convert to RGB (handles different formats)\n        image = Image.open(path).convert('RGB')\n        image.save(temp_filename, 'JPEG', quality=quality)\n        temp_image = Image.open(temp_filename)\n\n        ela_image = ImageChops.difference(image, temp_image)\n\n        extrema = ela_image.getextrema()\n        max_diff = max([ex[1] for ex in extrema])\n        if max_diff == 0:\n            max_diff = 1\n\n        scale = 255.0 / max_diff\n        ela_image = ImageEnhance.Brightness(ela_image).enhance(scale)\n        \n        # Clean up temp file\n        if os.path.exists(temp_filename):\n            os.remove(temp_filename)\n            \n        return ela_image\n    \n    except Exception as e:\n        print(f\"Error processing {path}: {e}\")\n        return None\n\n\ndef random_sample(path, extensions=None):\n    \"\"\"\n    Get a random sample image from a directory\n    \n    Args:\n        path: Directory path to search\n        extensions: List of file extensions to include, or None for all files\n    \n    Returns:\n        String path to a randomly selected file\n    \"\"\"\n    if extensions is None:\n        # Default to all your image formats\n        extensions = ['png', 'gif', 'jfif', 'jpg', 'webp', 'jpeg', \n                      'PNG', 'GIF', 'JFIF', 'JPG', 'WEBP', 'JPEG']\n    \n    items = []\n    \n    # If extensions is a list, iterate through each extension\n    if isinstance(extensions, list):\n        for ext in extensions:\n            items.extend(list(Path(path).glob(f'*.{ext}')))\n    # If extensions is a single string, use it directly\n    elif extensions:\n        items = list(Path(path).glob(f'*.{extensions}'))\n    # If no extensions specified, get all files\n    else:\n        items = list(Path(path).glob('*'))\n    \n    # Check if we found any files\n    if not items:\n        raise ValueError(f\"No files found in {path} with extensions {extensions}\")\n        \n    p = random.choice(items)\n    return p.as_posix()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T14:04:00.056651Z","iopub.execute_input":"2025-03-31T14:04:00.056983Z","iopub.status.idle":"2025-03-31T14:04:00.066891Z","shell.execute_reply.started":"2025-03-31T14:04:00.056956Z","shell.execute_reply":"2025-03-31T14:04:00.066026Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# View random sample from the dataset\np = random_sample('/kaggle/input/ppd-v2/ppd_v1/Thrips tabaci')\norig = cv2.imread(p)\norig = cv2.cvtColor(orig, cv2.COLOR_BGR2RGB) / 255.0\ninit_val = 100\ncolumns = 3\nrows = 3\n\nfig=plt.figure(figsize=(15, 10))\nfor i in range(1, columns*rows +1):\n    quality=init_val - (i-1) * 8\n    img = compute_ela_cv(path=p, quality=quality)\n    if i == 1:\n        img = orig.copy()\n    ax = fig.add_subplot(rows, columns, i) \n    ax.title.set_text(f'q: {quality}')\n    plt.imshow(img)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T14:04:05.982987Z","iopub.execute_input":"2025-03-31T14:04:05.983284Z","iopub.status.idle":"2025-03-31T14:04:07.599274Z","shell.execute_reply.started":"2025-03-31T14:04:05.983262Z","shell.execute_reply":"2025-03-31T14:04:07.598323Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Separate in train and test data\ntrain_df, test_df = train_test_split(image_df, test_size=0.2, shuffle=True, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T14:04:13.505300Z","iopub.execute_input":"2025-03-31T14:04:13.505595Z","iopub.status.idle":"2025-03-31T14:04:13.512449Z","shell.execute_reply.started":"2025-03-31T14:04:13.505571Z","shell.execute_reply":"2025-03-31T14:04:13.511554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_generator = ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.efficientnet_v2.preprocess_input,\n    validation_split=0.2\n)\n\ntest_generator = ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.efficientnet_v2.preprocess_input\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T14:04:17.630638Z","iopub.execute_input":"2025-03-31T14:04:17.630988Z","iopub.status.idle":"2025-03-31T14:04:17.635232Z","shell.execute_reply.started":"2025-03-31T14:04:17.630959Z","shell.execute_reply":"2025-03-31T14:04:17.634227Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split the data into three categories.\ntrain_images = train_generator.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=42,\n    subset='training'\n)\n\nval_images = train_generator.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=42,\n    subset='validation'\n)\n\ntest_images = test_generator.flow_from_dataframe(\n    dataframe=test_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T14:04:28.450529Z","iopub.execute_input":"2025-03-31T14:04:28.450858Z","iopub.status.idle":"2025-03-31T14:04:28.571697Z","shell.execute_reply.started":"2025-03-31T14:04:28.450833Z","shell.execute_reply":"2025-03-31T14:04:28.571030Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data Augmentation Step\naugment = tf.keras.Sequential([\n  tf.keras.layers.Resizing(224, 224),\n  tf.keras.layers.Rescaling(1./255),\n  tf.keras.layers.RandomFlip(\"horizontal\"),\n  tf.keras.layers.RandomRotation(0.1),\n  tf.keras.layers.RandomZoom(0.1),\n  tf.keras.layers.RandomContrast(0.1),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T14:04:32.862453Z","iopub.execute_input":"2025-03-31T14:04:32.862793Z","iopub.status.idle":"2025-03-31T14:04:32.880339Z","shell.execute_reply.started":"2025-03-31T14:04:32.862765Z","shell.execute_reply":"2025-03-31T14:04:32.879592Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the pretained model\npretrained_model = tf.keras.applications.efficientnet_v2.EfficientNetV2L(\n    input_shape=(224, 224, 3),\n    include_top=False,\n    weights='imagenet',\n    pooling='max'\n)\n\npretrained_model.trainable = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T14:04:36.240841Z","iopub.execute_input":"2025-03-31T14:04:36.241146Z","iopub.status.idle":"2025-03-31T14:04:41.241907Z","shell.execute_reply.started":"2025-03-31T14:04:36.241123Z","shell.execute_reply":"2025-03-31T14:04:41.241223Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create checkpoint callback\ncheckpoint_path = \"peanut_pests_classification_model_checkpoint.weights.h5\"\ncheckpoint_callback = ModelCheckpoint(checkpoint_path,\n                                     save_weights_only=True,\n                                     monitor=\"val_accuracy\",\n                                     save_best_only=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T14:04:45.258566Z","iopub.execute_input":"2025-03-31T14:04:45.258941Z","iopub.status.idle":"2025-03-31T14:04:45.262920Z","shell.execute_reply.started":"2025-03-31T14:04:45.258907Z","shell.execute_reply":"2025-03-31T14:04:45.262141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Setup EarlyStopping callback to stop training if model's val_loss doesn't improve for 3 epochs\nearly_stopping = EarlyStopping(monitor = \"val_loss\", # watch the val loss metric\n                               patience = 5,\n                               restore_best_weights = True) # if val loss decreases for 3 epochs in a row, stop training","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T14:04:49.027089Z","iopub.execute_input":"2025-03-31T14:04:49.027390Z","iopub.status.idle":"2025-03-31T14:04:49.031919Z","shell.execute_reply.started":"2025-03-31T14:04:49.027367Z","shell.execute_reply":"2025-03-31T14:04:49.030967Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\ninputs = pretrained_model.input\nx = augment(inputs)\n\nx = Dense(128, activation='relu')(pretrained_model.output)\nx = Dropout(0.45)(x)\nx = Dense(256, activation='relu')(x)\nx = Dropout(0.45)(x)\n\n\n# Change the final Dense layer to have 17 output units instead of 12\noutputs = Dense(14, activation='softmax')(x)\n\nmodel = Model(inputs=inputs, outputs=outputs)\n\nmodel.compile(\n    optimizer=Adam(0.00001),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# history = model.fit(\n#     train_images,\n#     steps_per_epoch=len(train_images),\n#     validation_data=val_images,\n#     validation_steps=len(val_images),\n#     epochs=100,\n#     callbacks=[\n#         early_stopping,\n#         create_tensorboard_callback(\"training_logs\", \n#                                     \"peanut_pests_classification\"),\n#         checkpoint_callback,\n#     ]\n# )\n\nhistory = model.fit(\n    train_images,\n    steps_per_epoch=len(train_images),\n    validation_data=val_images,\n    validation_steps=len(val_images),\n    epochs=100,\n    callbacks=[early_stopping, checkpoint_callback]  # Start with just one callback\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T14:14:02.740858Z","iopub.execute_input":"2025-03-31T14:14:02.741255Z","iopub.status.idle":"2025-03-31T14:43:24.905793Z","shell.execute_reply.started":"2025-03-31T14:14:02.741224Z","shell.execute_reply":"2025-03-31T14:43:24.904834Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\n\n# Keep your base pretrained model and augmentation\ninputs = pretrained_model.input\nx = augment(inputs)\n\n# Skip the GlobalAveragePooling2D since output is already 2D\n# Start directly with the first dense layer\nx = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(pretrained_model.output)\nx = BatchNormalization()(x)\nx = Dropout(0.3)(x)\n\n# Second dense block with batch normalization and reduced dropout\nx = Dense(512, activation='relu', kernel_regularizer=l2(0.001))(x)\nx = BatchNormalization()(x)\nx = Dropout(0.3)(x)\n\n# Output layer with 17 units (matching your target shape)\noutputs = Dense(14, activation='softmax')(x)\n\nmodel = Model(inputs=inputs, outputs=outputs)\n\n# Use a learning rate scheduler\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n\n# Compile with a slightly higher initial learning rate\nmodel.compile(\n    optimizer=Adam(0.0001),  # Increased from 0.00001\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Add the learning rate scheduler to your callbacks\nhistory = model.fit(\n    train_images,\n    steps_per_epoch=len(train_images),\n    validation_data=val_images,\n    validation_steps=len(val_images),\n    epochs=10,\n    callbacks=[early_stopping, checkpoint_callback, reduce_lr]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T15:14:42.021025Z","iopub.execute_input":"2025-03-31T15:14:42.021354Z","iopub.status.idle":"2025-03-31T15:19:53.745427Z","shell.execute_reply.started":"2025-03-31T15:14:42.021326Z","shell.execute_reply":"2025-03-31T15:19:53.744465Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\n\n# EntrÃ©e depuis le modÃ¨le prÃ©-entraÃ®nÃ©\ninputs = pretrained_model.input\nx = augment(inputs)\n\n# VÃ©rifier si GlobalAveragePooling2D est nÃ©cessaire\nif len(pretrained_model.output.shape) == 4:  # VÃ©rifie si la sortie est en 4D\n    x = GlobalAveragePooling2D()(pretrained_model.output)\nelse:\n    x = pretrained_model.output  # Utiliser directement la sortie s'il est dÃ©jÃ  en 2D\n\n# Premier bloc dense avec plus de neurones\nx = Dense(512, activation='relu', kernel_regularizer=l2(0.0005))(x)\nx = BatchNormalization()(x)\nx = Dropout(0.4)(x)\n\n# DeuxiÃ¨me bloc dense\nx = Dense(256, activation='relu', kernel_regularizer=l2(0.0005))(x)\nx = BatchNormalization()(x)\nx = Dropout(0.3)(x)\n\n# Couche de sortie\noutputs = Dense(14, activation='softmax')(x)\n\n# CrÃ©ation du modÃ¨le\nmodel = Model(inputs=inputs, outputs=outputs)\n\nfrom tensorflow.keras.callbacks import LearningRateScheduler\n\n# DÃ©finition du scheduler exponentiel\ndef lr_schedule(epoch, lr):\n    return lr * 0.9 if epoch > 3 else lr  # RÃ©duction progressive aprÃ¨s 3 epochs\n\nlr_scheduler = LearningRateScheduler(lr_schedule)\n\nmodel.compile(\n    optimizer=Adam(learning_rate=0.0003),  # AugmentÃ© pour accÃ©lÃ©rer l'apprentissage\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory = model.fit(\n    train_images,\n    validation_data=val_images,\n    epochs=20,\n    callbacks=[checkpoint_callback, lr_scheduler]\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T15:38:58.040306Z","iopub.execute_input":"2025-03-31T15:38:58.040619Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}